//! Types for elements that can be stored and manipulated using tensors.
#![allow(clippy::redundant_closure_call)]
use crate::backend::BackendStorage;
use crate::cpu::kernels::VecOps;
use crate::{CpuStorage, CpuStorageRef, Error, Result};

// ====================================================================================
// QUANTIZED TYPES - New Proc Macro Based System
// ====================================================================================
// The QuantizedDType enum and quantized_dispatch module are generated by the
// register_quantized_types! proc macro from candle-quantized-macros.
// 
// External crates define types like:
//   struct Q4_0;
//   impl Q4_0 {
//       const NAME: &'static str = "q4_0";
//       fn dequantize(data: &[u8], output: &mut [f32]) -> Result<()> { ... }
//       fn quantize(input: &[f32]) -> Result<Vec<u8>> { ... }
//       fn matmul(...) -> Result<Vec<u8>> { ... }
//       fn storage_size_in_bytes(num_elements: usize) -> usize { ... }
//   }
//
// Then invoke:
//   register_quantized_types!(Q4_0, Q4_1, Q8_0);
//
// This generates:
//   - pub enum QuantizedDType { Q4_0, Q4_1, Q8_0 }  // u8-sized, Copy
//   - pub mod quantized_dispatch { ... }             // O(1) dispatch via match
// ====================================================================================

// Placeholder type - will be replaced by proc macro generated enum
// For now, use a simple u8 enum that the proc macro will generate
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
#[repr(u8)]
pub enum QuantizedDType {
    // This will be generated by register_quantized_types! macro
    // Example:
    // Q4_0 = 0,
    // Q4_1 = 1,
    // Q8_0 = 2,
    __Placeholder = 255,
}

impl QuantizedDType {
    /// Get the name of this quantized type
    /// This will be generated by the proc macro
    pub fn name(self) -> &'static str {
        match self {
            QuantizedDType::__Placeholder => "__placeholder",
        }
    }
}

/// Quantized dispatch module - generated by register_quantized_types! macro
/// This provides O(1) dispatch to quantized implementations via match statements
pub mod quantized_dispatch {
    use crate::{Result, Error};
    
    /// Dequantize quantized data to f32 (CPU)
    #[allow(unused_variables)]
    pub fn dequantize_cpu(
        id: super::QuantizedDType,
        data: &[u8],
        output: &mut [f32],
    ) -> Result<()> {
        Err(Error::Msg(
            "No quantized types registered. Use register_quantized_types! macro.".into()
        ))
    }
    
    /// Quantize f32 data to quantized format (CPU)
    #[allow(unused_variables)]
    pub fn quantize_cpu(
        id: super::QuantizedDType,
        input: &[f32],
    ) -> Result<Vec<u8>> {
        Err(Error::Msg(
            "No quantized types registered. Use register_quantized_types! macro.".into()
        ))
    }
    
    /// Matrix multiplication: f32 × quantized → f32 (ONLY supported mixed precision)
    /// This is the common pattern for inference: f32_activations @ quantized_weights
    /// For other combinations (quantized × f32, quantized × quantized), use auto-dequantization
    #[allow(unused_variables)]
    pub fn matmul_cpu(
        lhs_f32: &[f32],
        lhs_shape: &[usize],
        rhs_id: super::QuantizedDType,
        rhs_data: &[u8],
        rhs_shape: &[usize],
    ) -> Result<Vec<f32>> {
        Err(Error::Msg(
            "No quantized types registered. Use register_quantized_types! macro.".into()
        ))
    }
}


/// The different types of elements allowed in tensors.
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum DType {
    // Floating-point 8 bits integer (4-bit exponent, 3-bit mantissa).
    F8E4M3,
    // Unsigned 8 bits integer.
    U8,
    // Unsigned 32 bits integer.
    U32,
    // Signed 64 bits integer.
    I64,
    // Brain floating-point using half precision (16 bits).
    BF16,
    // Floating-point using half precision (16 bits).
    F16,
    // Floating-point using single precision (32 bits).
    F32,
    // Floating-point using double precision (64 bits).
    F64,
    // Quantized Data - uses enum generated by register_quantized_types! macro
    Quantized(QuantizedDType),
}

#[derive(Debug, PartialEq, Eq)]
pub struct DTypeParseError(String);

impl std::fmt::Display for DTypeParseError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "cannot parse '{}' as a dtype", self.0)
    }
}

impl std::error::Error for DTypeParseError {}

impl std::str::FromStr for DType {
    type Err = DTypeParseError;
    fn from_str(s: &str) -> std::result::Result<Self, Self::Err> {
        match s {
            "u8" => Ok(Self::U8),
            "u32" => Ok(Self::U32),
            "i64" => Ok(Self::I64),
            "bf16" => Ok(Self::BF16),
            "f16" => Ok(Self::F16),
            "f32" => Ok(Self::F32),
            "f64" => Ok(Self::F64),
            "f8_e4m3" => Ok(Self::F8E4M3),
            _ => Err(DTypeParseError(s.to_string())),
        }
    }
}

impl DType {
    /// String representation for dtypes.
    /// For quantized types, returns the registered name or "quantized" if not found.
    pub fn as_str(&self) -> &'static str {
        match self {
            Self::U8 => "u8",
            Self::U32 => "u32",
            Self::I64 => "i64",
            Self::BF16 => "bf16",
            Self::F16 => "f16",
            Self::F32 => "f32",
            Self::F64 => "f64",
            Self::F8E4M3 => "f8_e4m3",
            Self::Quantized(id) => id.name(),
        }
    }

    /// The size used by each element in bytes, i.e. 1 for `U8`, 4 for `F32`.
    /// For quantized types, this returns 1 as they don't have a fixed size per element.
    /// Use the QuantizedType trait's size_in_bytes method instead for accurate sizing.
    pub fn size_in_bytes(&self) -> usize {
        match self {
            Self::U8 => 1,
            Self::F8E4M3 => 1,
            Self::U32 => 4,
            Self::I64 => 8,
            Self::BF16 => 2,
            Self::F16 => 2,
            Self::F32 => 4,
            Self::F64 => 8,
            Self::Quantized(_) => 1, // Placeholder, use QuantizedType::size_in_bytes for accurate sizing
        }
    }

    pub fn is_int(&self) -> bool {
        match self {
            Self::U8 | Self::U32 | Self::I64 => true,
            Self::BF16 | Self::F16 | Self::F32 | Self::F64 | Self::F8E4M3 | Self::Quantized(_) => false,
        }
    }

    pub fn is_float(&self) -> bool {
        match self {
            Self::U8 | Self::U32 | Self::I64 | Self::Quantized(_) => false,
            Self::BF16 | Self::F16 | Self::F32 | Self::F64 | Self::F8E4M3 => true,
        }
    }
    
    pub fn is_quantized(&self) -> bool {
        matches!(self, Self::Quantized(_))
    }
}

pub trait WithDType:
    Sized
    + Copy
    + num_traits::NumAssign
    + std::cmp::PartialOrd
    + std::fmt::Display
    + 'static
    + Send
    + Sync
    + std::any::Any
    + crate::cpu::kernels::VecOps
{
    const DTYPE: DType;

    fn from_f64(v: f64) -> Self;
    fn to_f64(self) -> f64;
    fn to_scalar(self) -> crate::scalar::Scalar;
    fn cpu_storage_ref(data: &[Self]) -> CpuStorageRef<'_>;
    fn to_cpu_storage_owned(data: Vec<Self>) -> CpuStorage;

    fn to_cpu_storage(data: &[Self]) -> CpuStorage {
        Self::to_cpu_storage_owned(data.to_vec())
    }

    fn cpu_storage_as_slice(s: &CpuStorage) -> Result<&[Self]>;
    fn cpu_storage_data(s: CpuStorage) -> Result<Vec<Self>>;
}

macro_rules! with_dtype {
    ($ty:ty, $dtype:ident, $from_f64:expr, $to_f64:expr) => {
        impl WithDType for $ty {
            const DTYPE: DType = DType::$dtype;

            fn from_f64(v: f64) -> Self {
                $from_f64(v)
            }

            fn to_f64(self) -> f64 {
                $to_f64(self)
            }

            fn to_scalar(self) -> crate::scalar::Scalar {
                crate::scalar::Scalar::$dtype(self)
            }

            fn cpu_storage_ref(data: &[Self]) -> CpuStorageRef<'_> {
                CpuStorageRef::$dtype(data)
            }

            fn to_cpu_storage_owned(data: Vec<Self>) -> CpuStorage {
                CpuStorage::$dtype(data)
            }

            fn cpu_storage_data(s: CpuStorage) -> Result<Vec<Self>> {
                match s {
                    CpuStorage::$dtype(data) => Ok(data),
                    _ => Err(Error::UnexpectedDType {
                        expected: DType::$dtype,
                        got: s.dtype(),
                        msg: "unexpected dtype",
                    }
                    .bt()),
                }
            }

            fn cpu_storage_as_slice(s: &CpuStorage) -> Result<&[Self]> {
                match s {
                    CpuStorage::$dtype(data) => Ok(data),
                    _ => Err(Error::UnexpectedDType {
                        expected: DType::$dtype,
                        got: s.dtype(),
                        msg: "unexpected dtype",
                    }
                    .bt()),
                }
            }
        }
    };
}
use float8::F8E4M3;
use half::{bf16, f16};

with_dtype!(u8, U8, |v: f64| v as u8, |v: u8| v as f64);
with_dtype!(u32, U32, |v: f64| v as u32, |v: u32| v as f64);
with_dtype!(i64, I64, |v: f64| v as i64, |v: i64| v as f64);
with_dtype!(f16, F16, f16::from_f64, f16::to_f64);
with_dtype!(bf16, BF16, bf16::from_f64, bf16::to_f64);
with_dtype!(f32, F32, |v: f64| v as f32, |v: f32| v as f64);
with_dtype!(f64, F64, |v: f64| v, |v: f64| v);
with_dtype!(F8E4M3, F8E4M3, |v: f64| F8E4M3::from_f64(v), |v: F8E4M3| v
    .to_f64());

impl VecOps for F8E4M3 {
    fn max(self, rhs: Self) -> Self {
        F8E4M3::max(self, rhs)
    }
    fn min(self, rhs: Self) -> Self {
        F8E4M3::min(self, rhs)
    }
}

pub trait IntDType: WithDType + num_traits::Bounded {
    fn is_true(&self) -> bool;
    fn as_usize(&self) -> usize;
}

impl IntDType for i64 {
    fn is_true(&self) -> bool {
        *self != 0
    }
    fn as_usize(&self) -> usize {
        *self as usize
    }
}

impl IntDType for u32 {
    fn is_true(&self) -> bool {
        *self != 0
    }
    fn as_usize(&self) -> usize {
        *self as usize
    }
}

impl IntDType for u8 {
    fn is_true(&self) -> bool {
        *self != 0
    }
    fn as_usize(&self) -> usize {
        *self as usize
    }
}

pub trait FloatDType: WithDType {}

impl FloatDType for f16 {}
impl FloatDType for bf16 {}
impl FloatDType for f32 {}
impl FloatDType for f64 {}
impl FloatDType for F8E4M3 {}
